{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_meaning = {1, 2, 7}\n",
    "# this word can refer to entities 1, 2, and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shorthair:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.personality = 'strange'\n",
    "\n",
    "    def speak(self):\n",
    "        print(self.name + ' goes, \"Meow.\"')\n",
    "\n",
    "Sauce = Shorthair('Saucy')\n",
    "Ketchup = Shorthair('Ketchup')\n",
    "Mayo = Shorthair('Mayo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = {Sauce, Ketchup, Mayo}\n",
    "# cat is a noun, and it can refer to the shorthair cats, Sauce, Ketchup, and Mayo\n",
    "# for this example, the set of possible entities that cat can refer to is Sauce, Ketchup, and Mayo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<__main__.Shorthair at 0x104e5f8d0>,\n",
       " <__main__.Shorthair at 0x104e61f90>,\n",
       " <__main__.Shorthair at 0x104e63510>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = {Sauce, Ketchup, Mayo, Mayo}\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/v_4svj7n17z2m3fg9lhk_xcw0000gp/T/ipykernel_11057/1457701759.py:6: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf')\n"
     ]
    }
   ],
   "source": [
    "from numpy import prod\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_probabilities(probs):\n",
    "    total = sum(probs)\n",
    "    normalized_probs = []\n",
    "    for p in probs:\n",
    "        normalized_probs.append(p / total)\n",
    "    return normalized_probs\n",
    "\n",
    "# sum every probability in a list of probs, e.g. [0.2, 0.3, 0.4, 0.5] \n",
    "# for every probability in that list, divide it by the calculated sum\n",
    "# 0.2 / (0.2 + 0.3 + 0.4 + 0.5) = 0.2 / 0.9 = 0.2 repeating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learners consider hypotheses of word meanings\n",
    "\n",
    "class Longhair:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.personality = 'fanciful'\n",
    "\n",
    "Hiccup = Longhair('Hiccup')\n",
    "\n",
    "cat_hypothesis_space = [{set[Shorthair]}, {set[Shorthair], set[Longhair]}]\n",
    "# the word cat can refer to the set of Shorthairs, OR the set of shorthairs and the set of longhairs, that includes Hiccup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0},\n",
       " {1},\n",
       " {2},\n",
       " {3},\n",
       " {4},\n",
       " {5},\n",
       " {6},\n",
       " {7},\n",
       " {8},\n",
       " {9},\n",
       " {10},\n",
       " {0, 1},\n",
       " {1, 2},\n",
       " {2, 3},\n",
       " {3, 4},\n",
       " {4, 5},\n",
       " {5, 6},\n",
       " {6, 7},\n",
       " {7, 8},\n",
       " {8, 9},\n",
       " {9, 10},\n",
       " {0, 1, 2},\n",
       " {1, 2, 3},\n",
       " {2, 3, 4},\n",
       " {3, 4, 5},\n",
       " {4, 5, 6},\n",
       " {5, 6, 7},\n",
       " {6, 7, 8},\n",
       " {7, 8, 9},\n",
       " {8, 9, 10},\n",
       " {0, 1, 2, 3},\n",
       " {1, 2, 3, 4},\n",
       " {2, 3, 4, 5},\n",
       " {3, 4, 5, 6},\n",
       " {4, 5, 6, 7},\n",
       " {5, 6, 7, 8},\n",
       " {6, 7, 8, 9},\n",
       " {7, 8, 9, 10},\n",
       " {0, 1, 2, 3, 4},\n",
       " {1, 2, 3, 4, 5},\n",
       " {2, 3, 4, 5, 6},\n",
       " {3, 4, 5, 6, 7},\n",
       " {4, 5, 6, 7, 8},\n",
       " {5, 6, 7, 8, 9},\n",
       " {6, 7, 8, 9, 10},\n",
       " {0, 1, 2, 3, 4, 5},\n",
       " {1, 2, 3, 4, 5, 6},\n",
       " {2, 3, 4, 5, 6, 7},\n",
       " {3, 4, 5, 6, 7, 8},\n",
       " {4, 5, 6, 7, 8, 9},\n",
       " {5, 6, 7, 8, 9, 10}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hypotheses = [{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},\n",
    "         {0,1},{1,2},{2,3},{3,4},{4,5},{5,6},{6,7},{7,8},{8,9},{9,10},\n",
    "         {0,1,2},{1,2,3},{2,3,4},{3,4,5},{4,5,6},{5,6,7},{6,7,8},{7,8,9},{8,9,10},\n",
    "         {0,1,2,3},{1,2,3,4},{2,3,4,5},{3,4,5,6},{4,5,6,7},{5,6,7,8},{6,7,8,9},{7,8,9,10},\n",
    "         {0,1,2,3,4},{1,2,3,4,5},{2,3,4,5,6},{3,4,5,6,7},{4,5,6,7,8},{5,6,7,8,9},{6,7,8,9,10},\n",
    "         {0,1,2,3,4,5},{1,2,3,4,5,6},{2,3,4,5,6,7},{3,4,5,6,7,8},{4,5,6,7,8,9},{5,6,7,8,9,10}]\n",
    "\n",
    "all_hypotheses\n",
    "# assumptions:\n",
    "# 1. there are 10 entities in this world, 0 - 10\n",
    "# 2. a word refers to at least one of those entities, and up to (inclusive) 6 entities\n",
    "# 3. hypotheses include entities that are clustered; in other words, 0 is clustered with entities 1, 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior(possible_hypotheses):\n",
    "    prior = []\n",
    "    for h in possible_hypotheses:\n",
    "        prior.append(1/len(possible_hypotheses))\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_prior(all_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [7, 7, 7, 7, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(data, hypothesis):\n",
    "    likelihoods = []\n",
    "    for data_item in data:\n",
    "        if data_item in hypothesis:\n",
    "            likelihood_this_item = 1/len(hypothesis)\n",
    "        else: \n",
    "            likelihood_this_item = 0\n",
    "        likelihoods.append(likelihood_this_item)\n",
    "    return prod(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(likelihood([0, 6, 7, 8], {7, 8, 9}))\n",
    "print(likelihood([6, 7], {7, 8, 9}))\n",
    "print(likelihood([6, 7], {7, 8, 9}))\n",
    "likelihood([6, 7], {6, 7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(data, possible_hypotheses, prior):\n",
    "    posteriors = []\n",
    "    for i in range(len(possible_hypotheses)):\n",
    "        h = possible_hypotheses[i]\n",
    "        prior_h = prior[i]\n",
    "        likelihood_h = likelihood(data, h)\n",
    "        posterior_h = prior_h * likelihood_h\n",
    "        posteriors.append(posterior_h)\n",
    "    return normalize_probabilities(posteriors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.6568863586599518,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19463299515850427,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08211079483249398,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.042040726954236926,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.024329124394813034,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prior = calculate_prior(all_hypotheses)\n",
    "posterior([0, 0, 1], all_hypotheses, my_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How does the amount of data influence the posterior distribution? For instance, is the posterior the same after seeing the data [0, 0, 1] and the data [0, 0, 1, 0, 0, 1]? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0} 0.0\n",
      "{1} 0.0\n",
      "{2} 0.0\n",
      "{3} 0.0\n",
      "{4} 0.0\n",
      "{5} 0.0\n",
      "{6} 0.0\n",
      "{7} 0.0\n",
      "{8} 0.0\n",
      "{9} 0.0\n",
      "{10} 0.0\n",
      "{0, 1} 0.6568863586599518\n",
      "{1, 2} 0.0\n",
      "{2, 3} 0.0\n",
      "{3, 4} 0.0\n",
      "{4, 5} 0.0\n",
      "{5, 6} 0.0\n",
      "{6, 7} 0.0\n",
      "{8, 7} 0.0\n",
      "{8, 9} 0.0\n",
      "{9, 10} 0.0\n",
      "{0, 1, 2} 0.19463299515850427\n",
      "{1, 2, 3} 0.0\n",
      "{2, 3, 4} 0.0\n",
      "{3, 4, 5} 0.0\n",
      "{4, 5, 6} 0.0\n",
      "{5, 6, 7} 0.0\n",
      "{8, 6, 7} 0.0\n",
      "{8, 9, 7} 0.0\n",
      "{8, 9, 10} 0.0\n",
      "{0, 1, 2, 3} 0.08211079483249398\n",
      "{1, 2, 3, 4} 0.0\n",
      "{2, 3, 4, 5} 0.0\n",
      "{3, 4, 5, 6} 0.0\n",
      "{4, 5, 6, 7} 0.0\n",
      "{8, 5, 6, 7} 0.0\n",
      "{8, 9, 6, 7} 0.0\n",
      "{8, 9, 10, 7} 0.0\n",
      "{0, 1, 2, 3, 4} 0.042040726954236926\n",
      "{1, 2, 3, 4, 5} 0.0\n",
      "{2, 3, 4, 5, 6} 0.0\n",
      "{3, 4, 5, 6, 7} 0.0\n",
      "{4, 5, 6, 7, 8} 0.0\n",
      "{5, 6, 7, 8, 9} 0.0\n",
      "{6, 7, 8, 9, 10} 0.0\n",
      "{0, 1, 2, 3, 4, 5} 0.024329124394813034\n",
      "{1, 2, 3, 4, 5, 6} 0.0\n",
      "{2, 3, 4, 5, 6, 7} 0.0\n",
      "{3, 4, 5, 6, 7, 8} 0.0\n",
      "{4, 5, 6, 7, 8, 9} 0.0\n",
      "{5, 6, 7, 8, 9, 10} 0.0\n"
     ]
    }
   ],
   "source": [
    "my_posterior1 = posterior([0, 0, 1], all_hypotheses, my_prior)\n",
    "for i in range(len(all_hypotheses)):\n",
    "    print(all_hypotheses[i], my_posterior1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0} 0.0\n",
      "{1} 0.0\n",
      "{2} 0.0\n",
      "{3} 0.0\n",
      "{4} 0.0\n",
      "{5} 0.0\n",
      "{6} 0.0\n",
      "{7} 0.0\n",
      "{8} 0.0\n",
      "{9} 0.0\n",
      "{10} 0.0\n",
      "{0, 1} 0.9018073901245205\n",
      "{1, 2} 0.0\n",
      "{2, 3} 0.0\n",
      "{3, 4} 0.0\n",
      "{4, 5} 0.0\n",
      "{5, 6} 0.0\n",
      "{6, 7} 0.0\n",
      "{8, 7} 0.0\n",
      "{8, 9} 0.0\n",
      "{9, 10} 0.0\n",
      "{0, 1, 2} 0.07917101916045172\n",
      "{1, 2, 3} 0.0\n",
      "{2, 3, 4} 0.0\n",
      "{3, 4, 5} 0.0\n",
      "{4, 5, 6} 0.0\n",
      "{5, 6, 7} 0.0\n",
      "{8, 6, 7} 0.0\n",
      "{8, 9, 7} 0.0\n",
      "{8, 9, 10} 0.0\n",
      "{0, 1, 2, 3} 0.014090740470695633\n",
      "{1, 2, 3, 4} 0.0\n",
      "{2, 3, 4, 5} 0.0\n",
      "{3, 4, 5, 6} 0.0\n",
      "{4, 5, 6, 7} 0.0\n",
      "{8, 5, 6, 7} 0.0\n",
      "{8, 9, 6, 7} 0.0\n",
      "{8, 9, 10, 7} 0.0\n",
      "{0, 1, 2, 3, 4} 0.0036938030699500374\n",
      "{1, 2, 3, 4, 5} 0.0\n",
      "{2, 3, 4, 5, 6} 0.0\n",
      "{3, 4, 5, 6, 7} 0.0\n",
      "{4, 5, 6, 7, 8} 0.0\n",
      "{5, 6, 7, 8, 9} 0.0\n",
      "{6, 7, 8, 9, 10} 0.0\n",
      "{0, 1, 2, 3, 4, 5} 0.001237047174382058\n",
      "{1, 2, 3, 4, 5, 6} 0.0\n",
      "{2, 3, 4, 5, 6, 7} 0.0\n",
      "{3, 4, 5, 6, 7, 8} 0.0\n",
      "{4, 5, 6, 7, 8, 9} 0.0\n",
      "{5, 6, 7, 8, 9, 10} 0.0\n"
     ]
    }
   ],
   "source": [
    "my_posterior2 = posterior([0, 0, 1, 0, 0, 1], all_hypotheses, my_prior)\n",
    "for i in range(len(all_hypotheses)):\n",
    "    print(all_hypotheses[i], my_posterior2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the data, [0, 0, 1], the hypothesis {0, 1} has a posterior probability of 0.6568863586599518. After seeing the data, [0, 0, 1, 0, 0, 1], the hypothesis {0, 1} has a posterior probability of 0.9018073901245205.\n",
    "The posterior probability of hypothesis {0, 1} increases after more information (i.e., data) is seen.\n",
    "The posterior probability of hypothesis {0, 1, 2} decreases after more information: after [0, 0, 1] it is 0.19463299515850427, and after [0, 0, 1, 0, 0, 1] it is 0.07917101916045172.\n",
    "In effect, the hypothesis that predicts fewer word meanings is preferred after more data.\n",
    "This reflects the size principle: \n",
    "\n",
    "$p(X|h)=[\\frac{1}{size(h)}]^n$\n",
    "\n",
    "Hypotheses that predict the word has fewer meanings (i.e., that have smaller extensions), assign exponentially greater probability to the same data than do hypotheses that predict the word has many more meanings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When are more specific word meanings preferred? When are more general word meanings preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most specific word meanings that explain (or, predict) the observed data are always preferred: they have fewer extensions, and if these extensions occur in the observed data, they are preferred over hypotheses that maintain extensions that did not occur in the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This code calculates a probability distribution over possible hypotheses given some data. If you had to commit to a single hypothesis, how would you choose one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would choose one that explains the data best. Given that I have no (known) preference for any of the possible hypotheses (in other words, equal priors assigned to each hypothesis), I would pick the hypothesis with the greatest likelihood: given the data, the hypothesis that best predicted the observed data. \n",
    "\n",
    "In this case, I pick the hypothesis with the greatest posterior probability. I pick the MAP hypothesis. \n",
    "\n",
    "If I do not pick the MAP hypothesis, I could pick a random hypothesis, despite the fact that there is one specific hypothesis with the greatest MAP. However, I would be more likely to select hypotheses that have greater posterior probabilities. In effect, I would more often select from hypotheses with posterior probabilities of 0.8, 0.9, etc, but I could select one with a low posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Do we have any kind of innateness in our model? Are there word meanings that our model learner could never learn, no matter what kind of data we gave them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If innateness is \"what the learner brings to the task of learning word meanings,\" then our model does have innateness. \n",
    "1. The prior : our learner does not prefer any hypothesis in all_hypotheses; they all have equal probability.\n",
    "2. The hypothesis space: there are hypotheses that we did not include, e.g. a) ones that predict the word refers to between 1 and 11 entities, b) ones that predict the word does not refer to clusters in the space of possible entities (e.g., {0, 11} is not a possible hypothesis). Given that we did not include these hypotheses, these hypotheses can never be learned by a learner. \n",
    "3. The likelihood: we assumed that the learner knows \n",
    "\n",
    "\n",
    "what data for each hypothesis should look like. In other words, given some labelling behaviour, the learner considers how likely that labelling behaviour is for each hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [Harder] We are assuming that candidate word meanings are nice and neat: a word refers to a set of entities that are contiguous in some sense (as represented by consecutive integers: i.e. our hypothesis space includes {3,4,5} but not {3,5} as a candidate meaning). Calculate the posterior probability distribution using the standard hypothesis space and the data [3,5]. Which hypothesis has the highest posterior probability? Now add a \"hypothesis with a hole in it\", {3,5} to the hypothesis space and recalculate the posterior. Which hypothesis has the highest posterior probability now, and why? Is there a potential problem here if we are trying to model learning? How would we change the model to disfavour this kind of \"hypothesis with a hole in it\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0} 0.0\n",
      "{1} 0.0\n",
      "{2} 0.0\n",
      "{3} 0.0\n",
      "{4} 0.0\n",
      "{5} 0.0\n",
      "{6} 0.0\n",
      "{7} 0.0\n",
      "{8} 0.0\n",
      "{9} 0.0\n",
      "{10} 0.0\n",
      "{0, 1} 0.0\n",
      "{1, 2} 0.0\n",
      "{2, 3} 0.0\n",
      "{3, 4} 0.0\n",
      "{4, 5} 0.0\n",
      "{5, 6} 0.0\n",
      "{6, 7} 0.0\n",
      "{8, 7} 0.0\n",
      "{8, 9} 0.0\n",
      "{9, 10} 0.0\n",
      "{0, 1, 2} 0.0\n",
      "{1, 2, 3} 0.0\n",
      "{2, 3, 4} 0.0\n",
      "{3, 4, 5} 0.23781212841854926\n",
      "{4, 5, 6} 0.0\n",
      "{5, 6, 7} 0.0\n",
      "{8, 6, 7} 0.0\n",
      "{8, 9, 7} 0.0\n",
      "{8, 9, 10} 0.0\n",
      "{0, 1, 2, 3} 0.0\n",
      "{1, 2, 3, 4} 0.0\n",
      "{2, 3, 4, 5} 0.13376932223543397\n",
      "{3, 4, 5, 6} 0.13376932223543397\n",
      "{4, 5, 6, 7} 0.0\n",
      "{8, 5, 6, 7} 0.0\n",
      "{8, 9, 6, 7} 0.0\n",
      "{8, 9, 10, 7} 0.0\n",
      "{0, 1, 2, 3, 4} 0.0\n",
      "{1, 2, 3, 4, 5} 0.08561236623067776\n",
      "{2, 3, 4, 5, 6} 0.08561236623067776\n",
      "{3, 4, 5, 6, 7} 0.08561236623067776\n",
      "{4, 5, 6, 7, 8} 0.0\n",
      "{5, 6, 7, 8, 9} 0.0\n",
      "{6, 7, 8, 9, 10} 0.0\n",
      "{0, 1, 2, 3, 4, 5} 0.059453032104637316\n",
      "{1, 2, 3, 4, 5, 6} 0.059453032104637316\n",
      "{2, 3, 4, 5, 6, 7} 0.059453032104637316\n",
      "{3, 4, 5, 6, 7, 8} 0.059453032104637316\n",
      "{4, 5, 6, 7, 8, 9} 0.0\n",
      "{5, 6, 7, 8, 9, 10} 0.0\n"
     ]
    }
   ],
   "source": [
    "my_posterior3 = posterior([3, 5], all_hypotheses, my_prior)\n",
    "for i in range(len(all_hypotheses)):\n",
    "    print(all_hypotheses[i], my_posterior3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "holed_hypothesis = {3, 5}\n",
    "all_hypotheses.append(holed_hypothesis)\n",
    "all_hypotheses\n",
    "\n",
    "my_prior2 = calculate_prior(all_hypotheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0} 0.0\n",
      "{1} 0.0\n",
      "{2} 0.0\n",
      "{3} 0.0\n",
      "{4} 0.0\n",
      "{5} 0.0\n",
      "{6} 0.0\n",
      "{7} 0.0\n",
      "{8} 0.0\n",
      "{9} 0.0\n",
      "{10} 0.0\n",
      "{0, 1} 0.0\n",
      "{1, 2} 0.0\n",
      "{2, 3} 0.0\n",
      "{3, 4} 0.0\n",
      "{4, 5} 0.0\n",
      "{5, 6} 0.0\n",
      "{6, 7} 0.0\n",
      "{8, 7} 0.0\n",
      "{8, 9} 0.0\n",
      "{9, 10} 0.0\n",
      "{0, 1, 2} 0.0\n",
      "{1, 2, 3} 0.0\n",
      "{2, 3, 4} 0.0\n",
      "{3, 4, 5} 0.1549186676994578\n",
      "{4, 5, 6} 0.0\n",
      "{5, 6, 7} 0.0\n",
      "{8, 6, 7} 0.0\n",
      "{8, 9, 7} 0.0\n",
      "{8, 9, 10} 0.0\n",
      "{0, 1, 2, 3} 0.0\n",
      "{1, 2, 3, 4} 0.0\n",
      "{2, 3, 4, 5} 0.087141750580945\n",
      "{3, 4, 5, 6} 0.087141750580945\n",
      "{4, 5, 6, 7} 0.0\n",
      "{8, 5, 6, 7} 0.0\n",
      "{8, 9, 6, 7} 0.0\n",
      "{8, 9, 10, 7} 0.0\n",
      "{0, 1, 2, 3, 4} 0.0\n",
      "{1, 2, 3, 4, 5} 0.05577072037180482\n",
      "{2, 3, 4, 5, 6} 0.05577072037180482\n",
      "{3, 4, 5, 6, 7} 0.05577072037180482\n",
      "{4, 5, 6, 7, 8} 0.0\n",
      "{5, 6, 7, 8, 9} 0.0\n",
      "{6, 7, 8, 9, 10} 0.0\n",
      "{0, 1, 2, 3, 4, 5} 0.03872966692486445\n",
      "{1, 2, 3, 4, 5, 6} 0.03872966692486445\n",
      "{2, 3, 4, 5, 6, 7} 0.03872966692486445\n",
      "{3, 4, 5, 6, 7, 8} 0.03872966692486445\n",
      "{4, 5, 6, 7, 8, 9} 0.0\n",
      "{5, 6, 7, 8, 9, 10} 0.0\n",
      "{3, 5} 0.34856700232378\n"
     ]
    }
   ],
   "source": [
    "my_posterior4 = posterior([3, 5], all_hypotheses, my_prior2)\n",
    "for i in range(len(all_hypotheses)):\n",
    "    print(all_hypotheses[i], my_posterior4[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06666666666666667, 0.06666666666666667, 0.8666666666666667]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_1 = {1, 2}\n",
    "hyp_2 = {1, 3}\n",
    "hyps = [hyp_1, hyp_2]\n",
    "hyp_3 = {2, 3}\n",
    "hyps.append(hyp_3)\n",
    "\n",
    "def calculate_prior2(possible_hypotheses):\n",
    "    prior = []\n",
    "    bad = []\n",
    "    good = []\n",
    "    bad_priors = []\n",
    "    good_priors = []\n",
    "    for h in possible_hypotheses:\n",
    "        for x in h:\n",
    "            if x == 1: \n",
    "                bad.append(h)\n",
    "        if h not in bad: \n",
    "            good.append(h)\n",
    "        if h in bad:\n",
    "            bad_priors.append((1/len(possible_hypotheses)) * 0.2)\n",
    "        bad_priors_sum = sum(bad_priors)\n",
    "        good_priors_sum = 1 - bad_priors_sum\n",
    "        for h in good:\n",
    "            good_priors.append(good_priors_sum/(len(good)))\n",
    "    for item in bad_priors:\n",
    "        prior.append(item)\n",
    "    for item in good_priors:\n",
    "        prior.append(item)\n",
    "    # return prior, bad, good, bad_priors, bad_priors_sum, good_priors_sum, good_priors, prior\n",
    "    return prior\n",
    "\n",
    "calculate_prior2(hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.25\n",
      "0.25\n",
      "0.125\n",
      "0.125\n",
      "0.125\n",
      "0.25\n",
      "0.0\n",
      "0.0\n",
      "0.25\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(likelihood([1, 2], hyp_1))\n",
    "print(likelihood([1, 3], hyp_2))\n",
    "print(likelihood([2, 3], hyp_3))\n",
    "\n",
    "print(likelihood([1, 2, 2], hyp_1))\n",
    "print(likelihood([1, 3, 3], hyp_2))\n",
    "print(likelihood([2, 3, 3], hyp_3))\n",
    "\n",
    "for h in hyps:\n",
    "    print(likelihood([1, 2], h))\n",
    "\n",
    "print(likelihood([1, 2], hyp_1))\n",
    "print(likelihood([1, 2], hyp_2))\n",
    "print(likelihood([1, 2], hyp_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06666666666666667, 0.06666666666666667, 0.8666666666666667]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_prior3 = calculate_prior2(hyps)\n",
    "my_prior3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2} 0.07142857142857142 0.06666666666666667\n",
      "{1, 3} 0.0 0.06666666666666667\n",
      "{2, 3} 0.9285714285714286 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "def posterior(data, possible_hypotheses, prior):\n",
    "    posteriors = []\n",
    "    for i in range(len(possible_hypotheses)):\n",
    "        h = possible_hypotheses[i]\n",
    "        prior_h = prior[i]\n",
    "        likelihood_h = likelihood(data, h)\n",
    "        posterior_h = prior_h * likelihood_h\n",
    "        posteriors.append(posterior_h)\n",
    "    return normalize_probabilities(posteriors)\n",
    "\n",
    "posterior([2], hyps, my_prior3)\n",
    "\n",
    "my_posterior5 = posterior([2], hyps, my_prior3)\n",
    "for i in range(len(hyps)):\n",
    "    print(hyps[i], my_posterior5[i], my_prior3[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [Harder] We are assuming (in the likelihood function) that all meanings of a word are equally likely to be encountered. What other kinds of assumptions might you make? How could you model those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could make the assumption that middle terms are prototype examples. Leftmost and rightmost entities are fringe cases; as we move to the middle item in the list (from the left or the right), entities more closely resemble the protypical example(s, if the number of items in the list is even).\n",
    "For practice, we could make the inverse assumption: items furthest from the center (from the left or right of the middle item(s) are prototype examples, and the middle item(s) are the most extreme fringe cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{2, 3}, {3, 4, 5}, {2, 3, 4}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_4 = {2, 3}\n",
    "hyp_5 = {3, 4, 5}\n",
    "hyp_6 = {2, 3, 4}\n",
    "\n",
    "hyps2 = []\n",
    "hyps2.append(hyp_4)\n",
    "hyps2.append(hyp_5)\n",
    "hyps2.append(hyp_6)\n",
    "hyps2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2, 3}, {3, 4, 5}, {2, 3, 4}]\n",
      "0.1875\n",
      "0.0\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(hyps2)\n",
    "\n",
    "def likelihood2(data, hypothesis):\n",
    "    likelihoods = []\n",
    "    max_likelihoods = []\n",
    "    for x in data:\n",
    "        if x in hypothesis:\n",
    "            likelihood_this_item = 1/len(hypothesis)\n",
    "            if x == max(hypothesis):\n",
    "                likelihood_this_item = (1/len(hypothesis)) * 3\n",
    "                # the likelihood of the max term is 3x more likely than any other term\n",
    "        else:\n",
    "            likelihood_this_item = 0\n",
    "        likelihoods.append(likelihood_this_item)\n",
    "    total = sum(likelihoods)\n",
    "    # the sum of 1/2 likelihood and the 3/2 likelihood\n",
    "    normed_likelihoods = []\n",
    "    for like in likelihoods:\n",
    "        normed_likelihoods.append (like / total)\n",
    "    # divide by the sum of the likelihoods\n",
    "    return prod(normed_likelihoods)\n",
    "    \n",
    "for h in hyps2:\n",
    "    print(likelihood2([2, 3], h))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2, 3}, {3, 4, 5}, {2, 3, 4}]\n",
      "0.1875\n",
      "0.0\n",
      "0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "print(hyps2)\n",
    "\n",
    "def likelihood3(data, hypothesis):\n",
    "    likelihoods = []\n",
    "    max_likelihoods = []\n",
    "    if max(hypothesis) in data:\n",
    "        for data_item in data:\n",
    "            if data_item in hypothesis:\n",
    "                likelihood_this_item = 1/len(hypothesis)\n",
    "                if data_item == max(hypothesis):\n",
    "                    likelihood_this_item = likelihood_this_item * 3\n",
    "                # the likelihood of the max term is 3x more likely than any other term\n",
    "            else:\n",
    "                likelihood_this_item = 0\n",
    "            max_likelihoods.append(likelihood_this_item)\n",
    "            total = sum(max_likelihoods)\n",
    "            # the sum of 1/2 likelihood and the 3/2 likelihood\n",
    "            normed_max_likelihoods = []\n",
    "            for like in max_likelihoods:\n",
    "                normed_max_likelihoods.append(like/total)\n",
    "                # divide by the sum of the likelihoods\n",
    "        return prod(normed_max_likelihoods)\n",
    "    else:\n",
    "        for data_item in data:\n",
    "            if data_item in hypothesis:\n",
    "                likelihood_this_item = 1/len(hypothesis)\n",
    "            else: \n",
    "                likelihood_this_item = 0\n",
    "            likelihoods.append(likelihood_this_item)\n",
    "        return prod(likelihoods)\n",
    "\n",
    "for h in hyps2:\n",
    "    print(likelihood3([2, 3], h))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{2, 3}, {1, 2}, {2, 3, 5}]\n",
      "0.1875\n",
      "0.0\n",
      "0.25\n",
      "0.1875\n",
      "0.0\n",
      "0.1111111111111111\n",
      "0.1875\n",
      "0.0\n",
      "0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "hyps3 = []\n",
    "\n",
    "hyp_7 = {2, 3}\n",
    "hyp_8 = {1, 2}\n",
    "hyp_9 = {2, 3, 5}\n",
    "\n",
    "hyps3.append(hyp_7)\n",
    "hyps3.append(hyp_8)\n",
    "hyps3.append(hyp_9)\n",
    "\n",
    "print(hyps3)\n",
    "\n",
    "for h in hyps2:\n",
    "    print(likelihood2([2, 3], h))\n",
    "\n",
    "for h in hyps2:\n",
    "    print(likelihood3([2, 3], h))\n",
    "\n",
    "for h in hyps3:\n",
    "    print(likelihood3([2, 3], h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRA BAYES PRACTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5]\n",
      "[0.3333333333333333, 0.5]\n",
      "{0, 1, 2} 0.5 0.3333333333333333 0.4\n",
      "{0, 1} 0.5 0.5 0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "# Priors\n",
    "\n",
    "def practice_prior(possible_hypotheses):\n",
    "    prior = []\n",
    "    for h in possible_hypotheses:\n",
    "        prior.append(1/len(possible_hypotheses))\n",
    "    return prior\n",
    "\n",
    "practice_hyps = [{0,1,2}, {0,1}]\n",
    "\n",
    "my_prior4 = practice_prior(practice_hyps)\n",
    "print(my_prior4)\n",
    "\n",
    "# Likelihoods\n",
    "\n",
    "def practice_likelihood(data, hypothesis):\n",
    "    likelihoods = []\n",
    "    for data_item in data:\n",
    "        if data_item in hypothesis:\n",
    "            likelihood_this_item = 1/len(hypothesis)\n",
    "        else: \n",
    "            likelihood_this_item = 0\n",
    "        likelihoods.append(likelihood_this_item)\n",
    "    return prod(likelihoods)\n",
    "\n",
    "var = []\n",
    "for h in practice_hyps:\n",
    "    var.append(practice_likelihood([0], h))\n",
    "print(var)\n",
    "\n",
    "# Posteriors\n",
    "\n",
    "def posterior(data, possible_hypotheses, prior):\n",
    "    posteriors = []\n",
    "    for i in range(len(possible_hypotheses)):\n",
    "        h = possible_hypotheses[i]\n",
    "        prior_h = prior[i]\n",
    "        likelihood_h = likelihood(data, h)\n",
    "        posterior_h = prior_h * likelihood_h\n",
    "        posteriors.append(posterior_h)\n",
    "    return normalize_probabilities(posteriors)\n",
    "\n",
    "my_posterior6 = posterior([0], practice_hyps, my_prior4)\n",
    "for i in range(len(practice_hyps)):\n",
    "    print(practice_hyps[i], my_prior4[i], var[i], my_posterior6[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
